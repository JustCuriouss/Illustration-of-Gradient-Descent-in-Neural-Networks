# Illustration-of-Gradient-Descent-in-Neural-Networks

Contents
1. Basic Gradient Descent Concepts
Initial Cost/Loss Function: Visualization of a 2D convex curve representing the loss function, with gradient arrows.
Step-by-Step Descent: Parameter updates over iterations toward the minimum.
2. Variants of Gradient Descent
Batch Gradient Descent: Shows the cost function decreasing smoothly using all data points.
Stochastic Gradient Descent (SGD): Demonstrates parameter updates with one data point at a time, leading to a noisier path.
Mini-Batch Gradient Descent: Balances between batch and SGD, offering a smoother descent with manageable computation.
3. Learning Rate and its Impact
Proper Learning Rate: Demonstrates a smooth and efficient convergence to the loss function minimum.
Large Learning Rate: Shows oscillations and possible divergence from the minimum.
Small Learning Rate: Slow and inefficient convergence toward the minimum.
4. Optimization Techniques
Momentum: Illustrates faster convergence by accelerating in the direction of the gradient.
RMSProp/Adam: Demonstrates adaptive learning rates for different parameters, leading to efficient convergence.
5. Overfitting and Underfitting
Overfitting: Visualization of a model with high training accuracy but poor generalization on validation data.
Underfitting: Demonstrates a model too simple to capture the underlying patterns of the data.
